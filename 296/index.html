<!DOCTYPE html><html lang="en-US"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" /><title>RBFormer: Improve Adversarial Robustness of Transformers by Robust Bias</title><link rel="stylesheet" href="../assets/css/style.css"><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png"></head><body><div class="wrapper"><section><center><a href="../"><img src="../images/bmvc-logo.png" width="200" class="figure-img img-responsive center-block"></a><br /><br /></center><h2 class="project-name" style="font-weight:normal; font-size: 167%;" align="center">RBFormer: Improve Adversarial Robustness of Transformers by Robust Bias</h2><br><h5 style="font-weight:normal" align="center"><autocolor>Hao Cheng (The Hong Kong University of Science and Technology(Guangzhou)),* Jinhao Duan (Drexel University), Hui Li (Samsung Research and Development Institute China Xi'an), Jiahang Cao (The Hong Kong University of Science and Technology (Guangzhou)), Ping Wang (Xi'an Jiaotong University), Lyutianyang Zhang (University of Washington), Jize Zhang (HKUST), Kaidi Xu (Drexel University), Renjing Xu (The Hong Kong University of Science and Technology (Guangzhou))</autocolor></h5><h5 style="font-weight:normal;" align="center"><a href="http://bmvc2022.mpi-inf.mpg.de/BMVC/" target="_blank" ><I><autocolor>The 34<sup>th</sup> British Machine Vision Conference</autocolor></I></a></h5><div class="cta"><a href="https://papers.bmvc2023.org/0296.pdf" role="button">PDF</a><a href="https://bmvc2022.mpi-inf.mpg.de/BMVC2023/0296_poster.pdf" role="button">Poster</a><a href="https://bmvc2022.mpi-inf.mpg.de/BMVC2023/0296_video.mp4" role="button">Video</a><a href="https://bmvc2022.mpi-inf.mpg.de/BMVC2023/0296_supp.zip" role="button">Supplementary</a><br></div><h2 id="abstract">Abstract</h2>Recently, there has been a surge of interest and attention in Transformer-based structures, such as Vision Transformer (ViT) and Vision Multilayer Perceptron (VMLP). Compared with the previous convolution-based structures, the Transformer-based structure under investigation showcases a comparable or superior performance under its distinctive attention-based input token mixer strategy. Introducing adversarial examples as a robustness consideration has had a profound and detrimental impact on the performance of well-established convolution-based structures. This inherent vulnerability to adversarial attacks has also been demonstrated in Transformer-based structures. In this paper, our emphasis lies on investigating the intrinsic robustness of the structure rather than introducing novel defense measures against adversarial attacks. To address the susceptibility to robustness issues, we employ a rational structure design approach to mitigate such vulnerabilities. Specifically, we enhance the adversarial robustness of the structure by increasing the proportion of high-frequency structural robust biases. As a result, we introduce a novel structure called Robust Bias Transformer-based Structure (RBFormer) that shows robust superiority compared to several existing baseline structures. Through a series of extensive experiments, RBFormer outperforms the original structures by a significant margin, achieving an impressive improvement of $+16.12\%$ and $+5.04\%$ across different evaluation criteria on CIFAR-10 and ImageNet-1k, respectively.<br><br><h2>Video</h2><center><iframe height="540" width="960" style="max-width:100%;max-height:100%;" src="https://bmvc2022.mpi-inf.mpg.de/BMVC2023/0296_video.mp4" frameborder="0" allow="encrypted-media" allowfullscreen></iframe></center><br><br><h2>Citation</h2><div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{Cheng_2023_BMVC,
author    = {Hao Cheng and Jinhao Duan and Hui Li and Jiahang Cao and Ping Wang and Lyutianyang Zhang and Jize Zhang and Kaidi Xu and Renjing Xu},
title     = {RBFormer: Improve Adversarial Robustness of Transformers by Robust Bias},
booktitle = {34th British Machine Vision Conference 2023, {BMVC} 2023, Aberdeen, UK, November 20-24, 2023},
publisher = {BMVA},
year      = {2023},
url       = {https://papers.bmvc2023.org/0296.pdf}
}
</code></pre></div></div><br><br><p><small>Copyright &copy 2023 <a href="https://britishmachinevisionassociation.github.io/" rel="noopener"><autocolor>The British Machine Vision Association and Society for Pattern Recognition</autocolor></a><br>The British Machine Vision Conference is organised by <a href="https://britishmachinevisionassociation.github.io/"><autocolor>The British Machine Vision Association and Society for Pattern Recognition</autocolor></a>. The Association is a Company limited by guarantee, No.2543446, and a non-profit-making body, registered in England and Wales as Charity No.1002307 (Registered Office: Dept. of Computer Science, Durham University, South Road, Durham, DH1 3LE, UK).</small></p><p><small><a href="https://imprint.mpi-klsb.mpg.de/inf/bmvc2022.mpi-inf.mpg.de" rel="noopener"><autocolor>Imprint</autocolor></a> | <a href="https://data-protection.mpi-klsb.mpg.de/inf/bmvc2022.mpi-inf.mpg.de?lang=en" rel="noopener"><autocolor>Data Protection</autocolor></a></small></p></section></div></body></html>